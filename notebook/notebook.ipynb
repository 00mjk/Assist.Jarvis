{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\home\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "import json\n",
    "import socket\n",
    "import time\n",
    "from time import mktime\n",
    "from datetime import datetime, date\n",
    "\n",
    "import os\n",
    "\n",
    "import psutil\n",
    "import requests\n",
    "from pytz import timezone\n",
    "from weather import Unit, Weather\n",
    "import multiprocessing\n",
    "\n",
    "import random\n",
    "import parsedatetime as pdt\n",
    "\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "from io import open\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.layers import LSTM, Dense, Embedding, Input\n",
    "from keras.models import Model, Sequential\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class DialogModel:\n",
    "    def __init__(self, encoder_input_data,\n",
    "                 decoder_input_data, decoder_target_data):\n",
    "        self.encoder_input_data = encoder_input_data\n",
    "        self.decoder_input_data = decoder_input_data\n",
    "        self.decoder_target_data = decoder_target_data\n",
    "\n",
    "        # intialize attributes for encoder, decoder model\n",
    "        self.encoder_inputs = None\n",
    "        self.decoder_inputs = None\n",
    "        self.decoder_outputs = None\n",
    "        self.decoder_latent_dim = None\n",
    "        self.history = None\n",
    "\n",
    "        # initializing attributes for inference model\n",
    "        self.encoder_model = None\n",
    "        self.decoder_model = None\n",
    "        self._decoder = None\n",
    "        self._decoder_dense = None\n",
    "        self.inference_ready = False\n",
    "\n",
    "    def encoder(self, input_len, latent_dim=128, activation='softmax'):\n",
    "        self.encoder_inputs = Input(shape=(None, input_len))\n",
    "        encoder = LSTM(latent_dim, activation=activation, return_state=True)\n",
    "        _, state_h, state_c = encoder(self.encoder_inputs)\n",
    "        self.encoder_states = [state_h, state_c]\n",
    "\n",
    "    def decoder(self, output_len, latent_dim=128, activation='tanh'):\n",
    "        self.decoder_latent_dim = latent_dim\n",
    "        self.decoder_inputs = Input(shape=(None, output_len))\n",
    "        self._decoder = LSTM(\n",
    "            self.decoder_latent_dim, activation=activation,\n",
    "            return_sequences=True, return_state=True)\n",
    "        decoder_outputs, _, _ = \\\n",
    "            self._decoder(self.decoder_inputs,\n",
    "                          initial_state=self.encoder_states)\n",
    "        self._decoder_dense = Dense(output_len, activation='softmax')\n",
    "        self.decoder_outputs = self._decoder_dense(decoder_outputs)\n",
    "\n",
    "    def train(self, optimizer='rmsprop', loss='categorical_crossentropy',\n",
    "              batch_size=64, epochs=100, validation=0.0, summary=False, name=''):\n",
    "        model = Model(\n",
    "            [self.encoder_inputs, self.decoder_inputs], self.decoder_outputs)\n",
    "\n",
    "        if summary:\n",
    "            model.summary()\n",
    "\n",
    "        model.compile(optimizer, loss, metrics=['mae', 'categorical_accuracy'])\n",
    "        self.history = model.fit(\n",
    "            [self.encoder_input_data, self.decoder_input_data],\n",
    "            self.decoder_target_data, batch_size=batch_size,\n",
    "            epochs=epochs, validation_split=validation)\n",
    "        \n",
    "        model.save(name + '.dialog.model.h5')\n",
    "        \n",
    "    def load(self, name = ''):\n",
    "        model = Model(\n",
    "            [self.encoder_inputs, self.decoder_inputs], self.decoder_outputs)\n",
    "        model.load_weights(name + '.dialog.model.h5')\n",
    "\n",
    "    def build_inference_model(self):\n",
    "        self.encoder_model = Model(self.encoder_inputs, self.encoder_states)\n",
    "\n",
    "        latent_dim = self.decoder_latent_dim\n",
    "        decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "        decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "        decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "        decoder_outputs, state_h, state_c = self._decoder(\n",
    "            self.decoder_inputs, initial_state=decoder_states_inputs)\n",
    "        decoder_states = [state_h, state_c]\n",
    "        decoder_outputs = self._decoder_dense(decoder_outputs)\n",
    "        self.decoder_model = Model(\n",
    "            [self.decoder_inputs] + decoder_states_inputs,\n",
    "            [decoder_outputs] + decoder_states)\n",
    "\n",
    "        self.inference_ready = True\n",
    "\n",
    "    def decode(self, input_sequence, output_word_model, max_sentence_len):\n",
    "        if not self.inference_ready:\n",
    "            self.build_inference_model()\n",
    "\n",
    "        state_values = self.encoder_model.predict(input_sequence)\n",
    "        output_len = output_word_model.n_words\n",
    "        target_seq = np.zeros((1, 1, output_len))\n",
    "        target_seq[0, 0, output_word_model.word2index['SOS']] = 1\n",
    "        stop_cond = False\n",
    "        decoded_sent = []\n",
    "        confidence = 0\n",
    "        while not stop_cond:\n",
    "            output_tokens, h, c = \\\n",
    "                self.decoder_model.predict([target_seq] + state_values)\n",
    "            sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "            word = output_word_model.index2word[sampled_token_index]\n",
    "            if word == 'EOS' or len(decoded_sent) > max_sentence_len:\n",
    "                stop_cond = True\n",
    "            else:\n",
    "                confidence += max(output_tokens[0, -1, :])\n",
    "                decoded_sent.append(word)\n",
    "            target_seq = np.zeros((1, 1, output_len))\n",
    "            target_seq[0, 0, output_word_model.word2index[word]] = 1\n",
    "            state_values = [h, c]\n",
    "        return \" \".join(decoded_sent), confidence/len(decoded_sent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def IntentModel(model):\n",
    "    model = model.lower()\n",
    "    if model == \"onehot\":\n",
    "        return OneHotModel\n",
    "    elif model == \"embeddings\":\n",
    "        return EmbeddingsModel\n",
    "    else:\n",
    "        print(\"{} does not exist\".format(model))\n",
    "        return None\n",
    "\n",
    "\n",
    "class BaseIntentModel:\n",
    "    def __init__(self):\n",
    "        self.x_train = None\n",
    "        self.y_train = None\n",
    "        self.model = None\n",
    "        self.history = None\n",
    "\n",
    "    def train(self, optimizer='rmsprop', loss='categorical_crossentropy',\n",
    "              batch_size=5, epochs=100, validation=0.0, summary=False, name = ''):\n",
    "        if summary:\n",
    "            self.model.summary()\n",
    "\n",
    "        self.model.compile(optimizer, loss, metrics=['mae', 'categorical_accuracy'])\n",
    "        self.history = self.model.fit(\n",
    "            self.x_train, self.y_train,\n",
    "            batch_size=batch_size, epochs=epochs,\n",
    "            validation_split=validation)\n",
    "        if not name == '':\n",
    "          self.model.save_weights(name + '.intent.model.h5')\n",
    "        \n",
    "    def load(self, name):\n",
    "      self.model.load_weights(name + '.intent.model.h5')\n",
    "      \n",
    "    def save(self, name):\n",
    "      self.model.save_weights(name + '.intent.model.h5')\n",
    "\n",
    "    def decode(self, input_sequence, output_word_model):\n",
    "        output_tokens = self.model.predict(input_sequence)\n",
    "        token_index = np.argmax(output_tokens[0])\n",
    "        intent = output_word_model.index2word[token_index]\n",
    "        confidence = max(output_tokens[0])\n",
    "        return intent, confidence\n",
    "\n",
    "\n",
    "class OneHotModel(BaseIntentModel):\n",
    "    def __init__(self, x_train, y_train,\n",
    "                 input_len, output_len,\n",
    "                 latent_dim=128, activation='tanh'):\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.model = \\\n",
    "            self.build_model(input_len, output_len, latent_dim, activation)\n",
    "\n",
    "    def build_model(self, input_len, output_len, latent_dim, activation):\n",
    "        model = Sequential()\n",
    "        model.add(\n",
    "            LSTM(latent_dim, activation=activation,\n",
    "                 input_shape=(None, input_len)))\n",
    "        model.add(Dense(36, activation = 'relu'))\n",
    "        model.add(Dense(output_len, activation='softmax'))\n",
    "        return model\n",
    "\n",
    "\n",
    "class EmbeddingsModel(BaseIntentModel):\n",
    "    def __init__(self, x_train, y_train,\n",
    "                 input_dim, output_dim,\n",
    "                 latent_dim=128, activation='tanh'):\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.model = \\\n",
    "            self.build_model(input_dim, output_dim, latent_dim, activation)\n",
    "\n",
    "    def build_model(self, input_dim, output_dim, latent_dim, activation):\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(input_dim, output_dim,))\n",
    "        model.add(\n",
    "            LSTM(latent_dim, activation=activation,\n",
    "                 input_shape=(None, input_dim)))\n",
    "        model.add(Dense(36, activation = 'relu'))\n",
    "        model.add(Dense(output_dim, activation='softmax'))\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PAD = \"PAD\"\n",
    "SOS = \"SOS\"\n",
    "EOS = \"EOS\"\n",
    "UNK = \"UNK\"\n",
    "\n",
    "\n",
    "class WordModel:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {PAD: 0, SOS: 1, EOS: 2, UNK: 3}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: PAD, 1: SOS, 2: EOS, 3: UNK}\n",
    "        self.n_words = 4  # count for default tokens\n",
    "\n",
    "    def add_sentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.add_word(word)\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "\n",
    "# helper functions to prepocess\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "\n",
    "def normalize_string(s):\n",
    "    s = unicode_to_ascii(s.lower().rstrip())\n",
    "    s = re.sub(r\"([!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.=!?{}:]+\", r\" \", s)\n",
    "    if s[-1] == \".\":\n",
    "        s = s[:-1]+\" .\"\n",
    "    return s\n",
    "\n",
    "\n",
    "def filter_pair(p):\n",
    "    \"\"\"\" filter sequence by MAX_LENGTH \"\"\"\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH\n",
    "\n",
    "\n",
    "def filter_pairs(pairs):\n",
    "    \"\"\" wrapper that filter sequence pairs by MAX_LENGTH \"\"\"\n",
    "    return [pair for pair in pairs if filter_pair(pair)]\n",
    "\n",
    "\n",
    "def get_data_pairs_from_json(_input, _output, filename):\n",
    "    \"\"\" \"\"\"\n",
    "    with open(filename, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    pairs = []\n",
    "    for pair in data:\n",
    "        pairs.append(\n",
    "            [normalize_string(pair[_input]), normalize_string(pair[_output])])\n",
    "    input_word_model = WordModel(_input)\n",
    "    output_word_model = WordModel(_output)\n",
    "    return input_word_model, output_word_model, pairs\n",
    "\n",
    "\n",
    "def prepare_json_data(_input, _output, filename, max_length=15, name = \"\"):\n",
    "    \"\"\" prepares data for processing. Expect json structure to follow...\n",
    "\n",
    "            {\n",
    "                \"input\": \"input string\"\n",
    "                \"output\": \"output string\"\n",
    "            }\n",
    "\n",
    "        Args:\n",
    "            _input (str): string of the key for model input\n",
    "            _output (str): string of the key for model output\n",
    "            filename (str): string of absolute path of file\n",
    "            max_length (int): Maximum length of the sequence\n",
    "\n",
    "        Returns:\n",
    "            input_word_model (WordModel): object containing input defintiions\n",
    "            output_word_model (WordModel): object containing output definitions\n",
    "            pairs (list<str>): [input, output] sentences\n",
    "    \"\"\"\n",
    "    global MAX_LENGTH\n",
    "    MAX_LENGTH = max_length\n",
    "\n",
    "    input_word_model, output_word_model, pairs = \\\n",
    "        get_data_pairs_from_json(_input, _output, filename)\n",
    "    if name == \"\":\n",
    "        print(\"READ %s sentence pairs\" % len(pairs))\n",
    "    else:\n",
    "        print(name, \"\\n\", \"\\t\", \"READ %s sentence pairs\" % len(pairs))\n",
    "    pairs = filter_pairs(pairs)\n",
    "    if name == \"\":\n",
    "        print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "        print(\"Counting words...\")\n",
    "    else:\n",
    "        print(\"\\tTrimmed to %s sentence pairs\" % len(pairs))\n",
    "        print(\"\\tCounting words...\")\n",
    "    for pair in pairs:\n",
    "        input_word_model.add_sentence(pair[0])\n",
    "        output_word_model.add_sentence(pair[1])\n",
    "    if name == \"\":\n",
    "        print(\"Counted Words:\")\n",
    "        print(input_word_model.name, input_word_model.n_words)\n",
    "        print(output_word_model.name, output_word_model.n_words)\n",
    "    else:\n",
    "        print(\"\\tCounted Words:\")\n",
    "        print(\"\\t\", input_word_model.name, input_word_model.n_words)\n",
    "        print(\"\\t\", output_word_model.name, output_word_model.n_words)\n",
    "    return input_word_model, output_word_model, pairs\n",
    "\n",
    "\n",
    "def pad_sequence(sequence, seq_len):\n",
    "    pad_len = (seq_len - 2) - len(sequence.split())\n",
    "    pads = [PAD for i in range(pad_len)]\n",
    "    _seq = \" \".join([SOS, *sequence.split()[:seq_len-2], EOS, *pads])\n",
    "    assert len(_seq.split()) == seq_len\n",
    "    return _seq\n",
    "\n",
    "\n",
    "def pad_sequences(sequences, seq_len):\n",
    "    \"\"\" adds PAD, SOS, EOS, UNK to sequence \"\"\"\n",
    "    padded_seqs = []\n",
    "    for seq in sequences:\n",
    "        _seq = pad_sequence(seq, seq_len)\n",
    "        padded_seqs.append(_seq.split())\n",
    "    return padded_seqs\n",
    "\n",
    "\n",
    "def integer_encode(sequences, word_model, seq_len):\n",
    "    \"\"\" returns nparray index encoded sequences \"\"\"\n",
    "    encoded_list = np.zeros((len(sequences), seq_len))\n",
    "    for i, seq in enumerate(sequences):\n",
    "        for j, word in enumerate(seq):\n",
    "            if word not in word_model.word2index:\n",
    "                word = UNK\n",
    "            encoded_list[i, j] = word_model.word2index[word]\n",
    "    return encoded_list\n",
    "\n",
    "\n",
    "def one_hot_encode(sequences, word_model, seq_len):\n",
    "    \"\"\" returns nparray with one hot encoded sequences \"\"\"\n",
    "    one_hot = np.zeros((len(sequences), seq_len, word_model.n_words))\n",
    "    for i, seq in enumerate(sequences):\n",
    "        for j, word in enumerate(seq):\n",
    "            if word != PAD:\n",
    "                if word not in word_model.word2index:\n",
    "                    word = UNK\n",
    "                one_hot[i, j, word_model.word2index[word]] = 1\n",
    "    return one_hot\n",
    "\n",
    "\n",
    "def one_hot_encode_target(sequences, word_model, seq_len):\n",
    "    \"\"\"\" encode target with an offset of 1 for decoder in seq2seq \"\"\"\n",
    "    one_hot = np.zeros((len(sequences), seq_len, word_model.n_words))\n",
    "    for i, seq in enumerate(sequences):\n",
    "        for j, word in enumerate(seq):\n",
    "            if word != PAD and j > 0:\n",
    "                one_hot[i, j-1, word_model.word2index[word]] = 1\n",
    "    return one_hot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Actions:\n",
    "  \n",
    "  def __init__(self):\n",
    "    self.Functions = {}\n",
    "    self.NullFunctions = {}\n",
    "    return\n",
    "  \n",
    "  def Add(self, Name, Function = None):\n",
    "    if Function == None:\n",
    "      self.NullFunctions[Name] = None\n",
    "    self.Functions[Name] = Function\n",
    "    #print(Name, ' : ', Function)\n",
    "    return\n",
    "  \n",
    "  def Func(self):\n",
    "    return self.Functions.items()\n",
    "  \n",
    "  def Print(self):\n",
    "    for key, value in self.Functions.items():\n",
    "      print('[', key , ' : ', value, ']')\n",
    "    return\n",
    "  \n",
    "  def ExecuteAction(self, intent, entities):\n",
    "    try:\n",
    "      if not len(entities.items()) == 0:\n",
    "        return self.Functions[intent](entities)\n",
    "      return self.Functions[intent]()\n",
    "    except:\n",
    "      # Add Exception, Error => log For why.did.skil.fail\n",
    "      return\n",
    "\n",
    "\n",
    "  def Mix(self, Entities, dialog):\n",
    "    for key, value in Entities.items():\n",
    "      r = key\n",
    "      r = r.replace('_', ' ')\n",
    "      r = \"{\" + r + \"}\"\n",
    "      dialog = dialog.replace(r, value)\n",
    "    return dialog\n",
    "  \n",
    "  def GetFeedbackDialog(self, intent, entities, dialog):\n",
    "    try:\n",
    "      if intent in self.NullFunctions.keys():\n",
    "        f = open('error.log', 'a')\n",
    "        f.writelines('\\n' + intent + '|actions.exceptions.NoActionFoundError')\n",
    "        f.close()\n",
    "        return ''.join(['Oops! looks like you are out of luck... ' , \n",
    "        'This project is just for educational purpose and is a demo,' ,\n",
    "        ' not a production ready app. Thus some of the capabilities are',\n",
    "        ' not yet added. Please add them manually or contact the' ,\n",
    "        ' developer at GitHub : arnavdas88 '])\n",
    "\n",
    "      if intent in self.Functions.keys():\n",
    "        if len(entities.items()) == 0:\n",
    "          entities = self.Functions[intent]()\n",
    "        else:\n",
    "          entities = self.Functions[intent](entities)\n",
    "      else:\n",
    "        return dialog\n",
    "    except:\n",
    "      # Add Exception, Error => log For why.did.skil.fail\n",
    "      error = str(sys.exc_info()[0]).replace('<class \\'','').replace('\\'>', '')\n",
    "      f = open('error.log', 'a')\n",
    "      f.writelines('\\n' + intent + '|' + error)\n",
    "      f.close()\n",
    "      return \"Oops! \" + error + \" occured.\"\n",
    "    '''\n",
    "    print('\\n\\t', entities.items(), '\\n')\n",
    "    '''\n",
    "\n",
    "    for key, value in entities.items():\n",
    "      r = key\n",
    "      r = r.replace('_', ' ')\n",
    "      r = \"{\" + r + \"}\"\n",
    "      value = str(value)\n",
    "      #print('[', key, ': ', value, ']')\n",
    "      dialog = dialog.replace(r, value)\n",
    "    return dialog\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NER:\n",
    "  def __init__(self, lang = 'en'):\n",
    "    self.nlp = spacy.load('en')\n",
    "    self.PropertyDiscription = {'GPE' : 'location', 'TIME' : 'time', 'DATE' : 'date'}\n",
    "    self.replaceWords = {}\n",
    "    return\n",
    "  \n",
    "  def GetFor(self, text = ''):\n",
    "    doc = self.nlp(text)\n",
    "    '''\n",
    "    for ent in doc.ents:\n",
    "        print(ent, ent.label_, [token.dep_ for token in ent])\n",
    "    '''\n",
    "    return doc.ents\n",
    "  \n",
    "  def AddCustomReplaceWords(self, words, _class):\n",
    "    for word in words:\n",
    "      self.replaceWords[word] = _class\n",
    "    return\n",
    "\n",
    "  def GetAllEntities(self, text = ''):\n",
    "    doc = self.nlp(text)\n",
    "    entities = {self.replaceWords[word] : word for word in text.split(' ') if word in self.replaceWords.keys()}\n",
    "    for key, value in entities.items():\n",
    "      r = key\n",
    "      r = r.replace('_', ' ')\n",
    "      r = \"{\" + r + \"}\"\n",
    "      value = str(value)\n",
    "      text = text.replace(r, value)\n",
    "    entities.update({self.PropertyDiscription[ent.label_] : str(ent) for ent in doc.ents if ent.label_ in self.PropertyDiscription.keys()})\n",
    "    for key, value in entities.items():\n",
    "      r = key\n",
    "      r = r.replace('_', ' ')\n",
    "      r = \"{\" + r + \"}\"\n",
    "      value = str(value)\n",
    "      text = text.replace(r, value)\n",
    "    return text, entities\n",
    "\n",
    "  def train(self):\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_ip(inputs = {}):\n",
    "    s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n",
    "    try:\n",
    "        # doesn't even have to be reachable\n",
    "        s.connect(('10.255.255.255', 1))\n",
    "        IP = s.getsockname()[0]\n",
    "    except:\n",
    "        IP = '127.0.0.1'\n",
    "    finally:\n",
    "        s.close()\n",
    "    return {'ip' : IP}\n",
    "\n",
    "def get_date(inputs = {}):\n",
    "  today = date.today()\n",
    "  month = today.strftime('%B')\n",
    "  day = today.day\n",
    "\n",
    "  if (3 < day < 21) or (23 < day < 31):\n",
    "    suffix = 'th'\n",
    "  else:\n",
    "    suffixes = {1: 'st', 2: 'nd', 3: 'rd'}\n",
    "    suffix = suffixes[day % 10]\n",
    "  \n",
    "  return {'day' : day, 'day_suffix' : suffix, 'month' : month}\n",
    "\n",
    "def get_time(inputs = {'location' : ''}):\n",
    "\n",
    "  place = ''\n",
    "  if not len(inputs.items()) == 0:\n",
    "    place = inputs['location']\n",
    "  \n",
    "  if place == '':\n",
    "    ts = time.time()\n",
    "    st = datetime.fromtimestamp(ts).strftime('%I %M %p')\n",
    "    tTime = st.split(' ')\n",
    "    t = str(int(tTime[0])) + ' ' + tTime[1] + ' ' + tTime[2]\n",
    "    return {'time' : t}\n",
    "\n",
    "  from tzwhere import tzwhere\n",
    "  place = place.replace(' ', '+')\n",
    "  search_details = 'https://geocoder.api.here.com/6.2/geocode.json?app_id=cb870QP7ugZzb4tWAcKI&app_code=d4xPcsr4E8fj_rzUNhkXgQ&searchtext=' + place\n",
    "\n",
    "  resp = requests.get(url=search_details, params='')\n",
    "  data = resp.json()\n",
    "  \n",
    "\n",
    "  tz = tzwhere.tzwhere()\n",
    "  lat = data['Response']['View'][0]['Result'][0]['Location']['DisplayPosition']['Latitude']\n",
    "  lon = data['Response']['View'][0]['Result'][0]['Location']['DisplayPosition']['Longitude']\n",
    "\n",
    "  \n",
    "  timeZoneStr = tz.tzNameAt(lat, lon)\n",
    "  timeZoneObj = timezone(timeZoneStr)\n",
    "  ts = datetime.now(timeZoneObj).strftime('%H:%M:%S')\n",
    "  return {'time' : ts}\n",
    "\n",
    "def get_cpu_max_process(inputs = {}):\n",
    "  name = ''\n",
    "  cpu = '0.0'\n",
    "  CPU_CORE = float(multiprocessing.cpu_count())\n",
    "  for _ in range(4):\n",
    "      pro = {float(proc.cpu_percent(interval=None) / CPU_CORE) : str(proc.name()) for proc in psutil.process_iter()}\n",
    "      cpu = max(pro.keys(), key=float)\n",
    "      name = pro[cpu]\n",
    "      #if not _name == 'pythonw.exe':\n",
    "      #cpu = _cpu\n",
    "      #name = _name\n",
    "      #print(name, ' : ', cpu, '%')\n",
    "  return {'name' : name, 'percent' : str(cpu) + '%'}\n",
    "\n",
    "def get_weather_current(inputs = {'location' : ''}):\n",
    "  \n",
    "  location = ''\n",
    "  if not len(inputs.items()) == 0:\n",
    "    location = inputs['location']\n",
    "\n",
    "  weather = Weather(unit=Unit.CELSIUS)\n",
    "  \n",
    "  if location == '':\n",
    "    r = requests.get(url = 'https://ipinfo.io/loc', params = '').text\n",
    "    lookup = weather.lookup_by_latlng(r.split(',')[0], r.split(',')[1])\n",
    "  else:\n",
    "    lookup = weather.lookup_by_location(location)\n",
    "    \n",
    "  condition = lookup.condition\n",
    "  location = lookup.location\n",
    "  forecast = lookup.forecast\n",
    "  return {'condition' : condition.text, 'temp_current' : condition.temp, 'scale' : 'C', 'diff_location' : location.city + ', ' + location.country, 'temp_max' : forecast[0].high, 'temp_min' : forecast[0].low }\n",
    "\n",
    "def get_weather_next_day(inputs = {'location' : ''}):\n",
    "  \n",
    "  location = ''\n",
    "  if not len(inputs.items()) == 0:\n",
    "    location = inputs['location']\n",
    "\n",
    "  weather = Weather(unit=Unit.CELSIUS)\n",
    "  \n",
    "  if location == '':\n",
    "    r = requests.get(url = 'https://ipinfo.io/loc', params = '').text\n",
    "    lookup = weather.lookup_by_latlng(r.split(',')[0], r.split(',')[1])\n",
    "  else:\n",
    "    lookup = weather.lookup_by_location(location)\n",
    "    \n",
    "  condition = lookup.condition\n",
    "  location = lookup.location\n",
    "  forecast = lookup.forecast\n",
    "  return {'condition' : forecast[1].text, 'diff_location' : location.city + ', ' + location.country, 'temp_max' : forecast[1].high, 'temp_min' : forecast[1].low }\n",
    "\n",
    "def set_alarm(inputs = {'time' : '', 'date' : ''}):\n",
    "  \n",
    "  if not 'time' in inputs.keys():\n",
    "    time = datetime.now()\n",
    "  else:\n",
    "    time = inputs['time'].replace('next', '+1')\n",
    "  \n",
    "  if not 'date' in inputs.keys():\n",
    "    date = datetime.today().strftime('%m/%d/%Y')\n",
    "  else:\n",
    "      if inputs['date'] == 'today':\n",
    "        date = datetime.today().strftime('%m/%d/%Y')\n",
    "      else:\n",
    "        date = inputs['date']\n",
    "  name = 'alarm'\n",
    "  str_dt = time + ' ' + date\n",
    "  p = pdt.Calendar()\n",
    "  _datetime = p.parse(str_dt)\n",
    "  dt = datetime.fromtimestamp(mktime(_datetime[0]))\n",
    "  time = str(dt.time())\n",
    "  date = str(dt.strftime('%m/%d/%Y'))\n",
    "  name = name + '|' + time + '|' + date\n",
    "  name = name.replace(':', '').replace('/', '').replace('|', '')\n",
    "  f = open('alarmList.txt', 'a')\n",
    "  f.write(name + '|' + time + '|' + date + '\\n')\n",
    "  f.close()\n",
    "  cwd = os.getcwd()\n",
    "  print(cwd)\n",
    "  comd = 'schtasks /create /tn \\\"' + name + '\\\" /tr ' + cwd + '\\\\alarm.bat /sc once /st ' + str(time) + ' /sd ' + str(date)\n",
    "  os.system(comd)\n",
    "  return {'weekday' : dt.strftime('%A'), 'hour' : dt.strftime('%I'), 'minute' : dt.strftime('%M'), 'am_pm' : dt.strftime('%p')}\n",
    "\n",
    "def remove_alarm(inputs = {}):\n",
    "    f = open('alarmList.txt', 'r')\n",
    "    lines = f.readlines()\n",
    "    f.close()\n",
    "    names = [line.split('|')[0] for line in lines]\n",
    "    template = 'schtasks /delete /tn '\n",
    "    [os.system(template + name + ' /F') for name in names]\n",
    "    f = open('alarmList.txt', 'w')\n",
    "    f.write('')\n",
    "    f.close()\n",
    "    return {}\n",
    "\n",
    "def list_alarm(inputs = {}):\n",
    "    f = open('alarmList.txt', 'r')\n",
    "    lines = f.readlines()\n",
    "    f.close()\n",
    "    names = [[line.split('|')[0], line.split('|')[1] + ' ' + line.split('|')[2]] for line in lines]\n",
    "    p = pdt.Calendar()\n",
    "    datetimelist = [(name[0], datetime.fromtimestamp(mktime(p.parse(name[1])[0]))) for name in names]\n",
    "    datetimelist = [Adatetime for (name, Adatetime) in datetimelist if Adatetime > datetime.now()]\n",
    "    datetimelist = [dt.strftime('%A, %d. %B %Y at %I:%M%p') for dt in datetimelist]\n",
    "    datetimelist = ', '.join(datetimelist)\n",
    "    if datetimelist == '':\n",
    "        return {'alarms' : 'No Alarms Found'}\n",
    "    return {'alarms' : datetimelist}\n",
    "\n",
    "def why_skill_failed(inputs = {'skill' : ''}):\n",
    "    f = open('error.log', 'r')\n",
    "    lines = f.readlines()\n",
    "    if lines == []:\n",
    "        return {'skill' : '', 'error' : 'No Error Found'}\n",
    "    if not inputs['skill'] == '':\n",
    "        lines = [line for line in lines if line.split('|')[0] == inputs['skill']]\n",
    "    error = lines[len(lines) - 1]\n",
    "    f.close()\n",
    "    return {'skill' : error.split('|')[0], 'error' : error.split('|')[1]}\n",
    "\n",
    "def get_jokes(inputs = {}):\n",
    "    joke=open('jokes.json').read()\n",
    "    joke = json.loads(joke)\n",
    "    jcount = joke.__len__()\n",
    "    joke = str(joke[random.randint(0, jcount)]['body'])\n",
    "    return {'joke' : joke}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#NER\n",
    "emodel = NER()\n",
    "\n",
    "#Dialog\n",
    "MAX_SEQ_LEN_IN = 15\n",
    "MAX_SEQ_LEN_OUT = 30\n",
    "\n",
    "dinput_word_model, doutput_word_model, dpairs = \\\n",
    "    prepare_json_data('input', 'output', './dialog.json', MAX_SEQ_LEN_OUT, name = \"Dialog\")\n",
    "\n",
    "dinput_seqs = [pair[0] for pair in dpairs]\n",
    "doutput_seqs = [pair[1] for pair in dpairs]\n",
    "\n",
    "#print([input_seq for input_seq in dinput_seqs])\n",
    "\n",
    "dpadded_input = pad_sequences(dinput_seqs, MAX_SEQ_LEN_IN)\n",
    "dpadded_output = pad_sequences(doutput_seqs, MAX_SEQ_LEN_OUT)\n",
    "\n",
    "done_hot_input = one_hot_encode(dpadded_input, dinput_word_model, MAX_SEQ_LEN_IN)\n",
    "done_hot_output = one_hot_encode(dpadded_output, doutput_word_model, MAX_SEQ_LEN_OUT)\n",
    "\n",
    "done_hot_target = one_hot_encode_target(dpadded_output, doutput_word_model, MAX_SEQ_LEN_OUT)\n",
    "\n",
    "\n",
    "dmodel = DialogModel(done_hot_input, done_hot_output, done_hot_target)\n",
    "dmodel.encoder(293)\n",
    "dmodel.decoder(152)\n",
    "dmodel.load(name = 'Full')\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "#Intent\n",
    "\n",
    "MAX_SEQ_LEN = 25\n",
    "iinput_word_model, ioutput_word_model, ipairs = \\\n",
    "    prepare_json_data('input', 'output', './intents.json', MAX_SEQ_LEN_IN, name = \"Intent\")\n",
    "\n",
    "input_seqs = [pair[0] for pair in ipairs]\n",
    "intent = [pair[1] for pair in ipairs]\n",
    "\n",
    "ipadded_input = pad_sequences(input_seqs, MAX_SEQ_LEN_OUT)\n",
    "\n",
    "ione_hot_input = one_hot_encode(ipadded_input, iinput_word_model, MAX_SEQ_LEN_IN)\n",
    "ione_hot_output = one_hot_encode([intent], ioutput_word_model, len(intent))[0]\n",
    "\n",
    "\n",
    "Imodel = IntentModel(\"onehot\")\n",
    "Imodel = Imodel(ione_hot_input, ione_hot_output, 244, 53)\n",
    "\n",
    "Imodel.load(name = 'Full')\n",
    "\n",
    "actionJar = Actions()\n",
    "\n",
    "\n",
    "\n",
    "emodel.AddCustomReplaceWords(list(set([intent[1] for intent in ipairs if '{'+intent[1]+'}' not in intent[0].split(' ')])), 'skill')\n",
    "\n",
    "# Adding the Actions to the intents...\n",
    "\n",
    "# actionJar.Add(INTENT, FUNCTION)\n",
    "\n",
    "actionJar.Add('ip', get_ip)\n",
    "actionJar.Add('time', get_time)\n",
    "actionJar.Add('date', get_date)\n",
    "actionJar.Add('cpu.max.process', get_cpu_max_process)\n",
    "actionJar.Add('weather.current', get_weather_current)\n",
    "actionJar.Add('weather.next.hour', get_weather_current)\n",
    "actionJar.Add('weather.next.day', get_weather_next_day)\n",
    "actionJar.Add('why.did.skill.fail', why_skill_failed)\n",
    "actionJar.Add('alarm.remove', remove_alarm)\n",
    "actionJar.Add('alarm.list', list_alarm)\n",
    "actionJar.Add('alarm.set', set_alarm)\n",
    "actionJar.Add('joke', get_jokes)\n",
    "\n",
    "# Actions that do needs but do not have an associated function yet\n",
    "actionJar.Add('say')\n",
    "actionJar.Add('pair')\n",
    "actionJar.Add('spell')\n",
    "actionJar.Add('show')\n",
    "actionJar.Add('open.path')\n",
    "actionJar.Add('ask')\n",
    "actionJar.Add('remove.all')\n",
    "actionJar.Add('add')\n",
    "actionJar.Add('open.palce')\n",
    "actionJar.Add('update.all')\n",
    "actionJar.Add('remove')\n",
    "actionJar.Add('update')\n",
    "actionJar.Add('math')\n",
    "actionJar.Add('alarm.stop')\n",
    "actionJar.Add('pandora.play')\n",
    "actionJar.Add('pandora.next')\n",
    "actionJar.Add('pandora.end')\n",
    "actionJar.Add('pandora.stop')\n",
    "actionJar.Add('cpu.total.usage')\n",
    "actionJar.Add('cpu.usage.application')\n",
    "actionJar.Add('mem.free')\n",
    "actionJar.Add('record.end')\n",
    "actionJar.Add('mem.usage.application')\n",
    "actionJar.Add('mem.max.process')\n",
    "actionJar.Add('record.begin')\n",
    "actionJar.Add('playback.begin')\n",
    "actionJar.Add('mem.total')\n",
    "actionJar.Add('mem.used')\n",
    "actionJar.Add('how.do.i.activate')\n",
    "\n",
    "print(\"All modules loaded successfully...\")\n",
    "print(\"Closed domain task oriented bot operational...\")\n",
    "print(\"\\n\")\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        _input = input(\"You : \")\n",
    "        ipadded_input = [pad_sequence(_input.lower(), MAX_SEQ_LEN_IN).split()]\n",
    "        ione_hot = one_hot_encode(ipadded_input, iinput_word_model, MAX_SEQ_LEN_IN)\n",
    "        intent, iconfidence = Imodel.decode(ione_hot, ioutput_word_model)\n",
    "        print(\"\\n\\t intent: \", intent, iconfidence)\n",
    "        \n",
    "        \n",
    "        _input, entities = emodel.GetAllEntities(_input)\n",
    "        print('\\t entity:', entities)\n",
    "        \n",
    "\n",
    "\n",
    "        '''\n",
    "        print(actionJar.ExecuteAction(intent, entities))\n",
    "        '''\n",
    "\n",
    "        _input = ':' + intent + ': ' + _input\n",
    "        #print('\\t', _input)\n",
    "        \n",
    "        dpadded_input = [pad_sequence(_input, len(_input.split(' '))).split()]\n",
    "        done_hot = one_hot_encode(dpadded_input, dinput_word_model, MAX_SEQ_LEN_IN)\n",
    "        dprediction, dconfidence = dmodel.decode(done_hot, doutput_word_model, MAX_SEQ_LEN_OUT)\n",
    "        #print(\"\\t\\tresponse: \", dprediction, dconfidence)\n",
    "\n",
    "        print('\\t  dialog:', dprediction, dconfidence, '\\n')\n",
    "        \n",
    "        print(\"Bot : \", actionJar.GetFeedbackDialog(intent, entities, dprediction))\n",
    "        \n",
    "        print('\\n')\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
